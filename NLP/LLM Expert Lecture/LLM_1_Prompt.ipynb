{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "O4BSypkY0hK-",
        "outputId": "df245a42-50c1-4d93-865f-2044d1a44841",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.9)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.24)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.26 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.26)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.8)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.26->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.26->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain) (3.9.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "#installing langchain\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Hugging Face models we need a Hugging Face Hub API token.\n",
        "\n",
        "We can find this by first getting an account at [HuggingFace.co](https://huggingface.co/) and clicking on our profile in the top-right corner > click *Settings* > click *Access Tokens* > click *New Token* > set *Role* to *write* > *Generate* > copy and paste the token below:"
      ],
      "metadata": {
        "id": "tWeRojInO9-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting Hugging face key\n",
        "from langchain.llms import HuggingFaceEndpoint\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass(\"Hugging Face Token: \")"
      ],
      "metadata": {
        "id": "dv5CSkbU5sfa",
        "outputId": "866b1f39-87e6-41b8-e979-44b3f538d10c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hugging Face Token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "epN09TGbpSIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model selection and Temperature settings\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id = \"google/flan-t5-xxl\" ,temperature = 0.5, max_new_tokens = 512\n",
        ")"
      ],
      "metadata": {
        "id": "oMzmp5fV4OE0",
        "outputId": "5b40d27f-e5e4-4714-f2d0-ffb426eb69f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts"
      ],
      "metadata": {
        "id": "DsEdny6QpWoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
        "Their superior performance over smaller models has made them incredibly\n",
        "useful for developers building NLP enabled applications. These models\n",
        "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
        "using the `openai` library, and via Cohere using the `cohere` library.\n",
        "\n",
        "Question: Which libraries and model providers offer LLMs?\n",
        "\n",
        "Answer: \"\"\""
      ],
      "metadata": {
        "id": "ejB4NSYt-6sD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_response = llm.invoke(prompt)\n",
        "print(llm_response)"
      ],
      "metadata": {
        "id": "4Rwu3BOQ_rL1",
        "outputId": "523eddd9-abc6-4580-80bf-182ea83697b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face's transformers library, via OpenAI using the openai library, and via Cohere using the cohere library\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
        "Their superior performance over smaller models has made them incredibly\n",
        "useful for developers building NLP enabled applications. These models\n",
        "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
        "using the `openai` library, and via Cohere using the `cohere` library.\n",
        "\n",
        "Question: Name some prominent celebrities that were invited for Ayodhya temple inauguration recently in 2024.\n",
        "\n",
        "Answer: \"\"\""
      ],
      "metadata": {
        "id": "GvhfAzskAVih"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_response = llm.invoke(prompt)\n",
        "print(llm_response)"
      ],
      "metadata": {
        "id": "uN5BNpjVH4_Z",
        "outputId": "9a16d8df-fe54-48c5-b1f5-9f9527e24019",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Template\n"
      ],
      "metadata": {
        "id": "aTtHlokX2bFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"query\",\"context\"],\n",
        "    template=template\n",
        ")"
      ],
      "metadata": {
        "id": "6j5Ii4Bem4AJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myprompt = prompt_template.format(\n",
        "        query=\"Which libraries and model providers offer LLMs?\",\n",
        "        context=\"Large Language Models (LLMs) are the latest models used in NLP. Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. These models can be accessed via Hugging Face's `transformers` library, via OpenAI using the `openai` library, and via Cohere using the `cohere` library.\"\n",
        "    )\n",
        "print(myprompt)"
      ],
      "metadata": {
        "id": "Wv-tZoeNaIjB",
        "outputId": "244bd232-4e0e-48ff-cbaf-56b1d8aa1d15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the question based on the context below. If the\n",
            "question cannot be answered using the information provided answer\n",
            "with \"I don't know\".\n",
            "\n",
            "Context: Large Language Models (LLMs) are the latest models used in NLP. Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. These models can be accessed via Hugging Face's `transformers` library, via OpenAI using the `openai` library, and via Cohere using the `cohere` library.\n",
            "\n",
            "Question: Which libraries and model providers offer LLMs?\n",
            "\n",
            "Answer: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.invoke(myprompt))"
      ],
      "metadata": {
        "id": "_iet8-XtaTyj",
        "outputId": "81fbb5e0-be79-4e25-c90a-a2634184c582",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face's transformers library, via OpenAI using the openai library, and via Cohere using the cohere library\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chains"
      ],
      "metadata": {
        "id": "rcH_3_13a1Q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt_template,llm=llm)\n",
        "query=\"Which libraries and model providers offer LLMs?\"\n",
        "context=\"Large Language Models (LLMs) are the latest models used in NLP. Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. These models can be accessed via Hugging Face's `transformers` library, via OpenAI using the `openai` library, and via Cohere using the `cohere` library.\"\n",
        "\n",
        "print(llm_chain.run({\"query\":query, \"context\":context}))"
      ],
      "metadata": {
        "id": "aLB4RWWiKOjx",
        "outputId": "fb46726b-9cf9-4019-a512-133f5bcaad28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face's transformers library, via OpenAI using the openai library, and via Cohere using the cohere library\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "PLoy1qGZR2ON"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt,llm=llm)"
      ],
      "metadata": {
        "id": "RHzFFYjvR8cG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who is your creator?\"\n",
        "print(question)\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "sKAaAM13SAvH",
        "outputId": "c4ba014e-ff33-4176-8930-b25616c9d299",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who is your creator?\n",
            "God is the creator of all things. God created you. The answer: God.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who is the CEO of LangChain open source framework?\"\n",
        "print(question)\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "s_5J8VnSSRs2",
        "outputId": "ffc42b17-7977-4fdb-f539-1140057aa7da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who is the CEO of LangChain open source framework?\n",
            "LangChain is an open source framework for distributed ledger technology. LangChain was founded by John McAfee and David Evans in 2014. John McAfee is the CEO of McAfee Enterprises. The answer: John McAfee.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is 2*5?\"\n",
        "print(question)\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "fYsth-DMLHX7",
        "outputId": "229d67d1-ceb3-44f0-99ae-1070ab9acbe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is 2*5?\n",
            "The product of two and five is 10. The answer: 10.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is 2**5?\"\n",
        "print(question)\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "qzWy33pvSwnN",
        "outputId": "19dc31fa-2e84-4998-c056-55677a637ddc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is 2**5?\n",
            "2**5 is equal to 25. The answer: 25.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import ConversationChain\n",
        "conversation = ConversationChain(llm=llm,verbose=True)\n"
      ],
      "metadata": {
        "id": "z6AQQyGQiI1Y"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Alice has a parrot.\")\n"
      ],
      "metadata": {
        "id": "tn_BDM1YOod_",
        "outputId": "1e30e932-edd7-42dd-8a76-ef2bd673864b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Alice has a parrot.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I have a parrot named Fanny.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Bob has two cats.\")\n"
      ],
      "metadata": {
        "id": "ldmPf5mdOpu1",
        "outputId": "8440d0d7-2e4a-4074-ff27-8c2d909b284a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Alice has a parrot.\n",
            "AI: I have a parrot named Fanny.\n",
            "Human: Bob has two cats.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I have a cat named Fanny.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"How many pets do Alice and Bob have?\")"
      ],
      "metadata": {
        "id": "gyez9m8hisb-",
        "outputId": "639f05e2-e6cf-4dea-de7f-ddb12c11fa56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Alice has a parrot.\n",
            "AI: I have a parrot named Fanny.\n",
            "Human: Bob has two cats.\n",
            "AI: I have a cat named Fanny.\n",
            "Human: How many pets do Alice and Bob have?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3 pets.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    }
  ]
}